
# SemEval-2026 Task 13 â€“ Full System (Subtasks A, B, C)

This repository contains our complete, modular solution for **SemEval-2026 Task 13**, covering:

- **Subtask A â€” Binary Machine-Generated Code Detection**
- **Subtask B â€” Multi-Class LLM Family Authorship Detection**
- **Subtask C â€” Hybrid + Adversarial Code Classification**

The project includes:

- TFâ€“IDF + classical ML baselines  
- Transformer-based models (e.g., **BERT**, **CodeBERT**, **CodeT5-small**)  
- Shared utilities for data loading, metrics, and plotting  
- Experiment scripts for training, evaluation, and ensembling  
- Inference pipelines for generating competition-ready submission files  

All three subtasks follow the **same structure and workflow**.

---

## ðŸ“Œ Task Overview

### ðŸ”¹ Subtask A â€” Binary Machine-Generated Code Detection

**Goal:**  
Given a code snippet, determine whether it is:

- **Human-written**, or  
- **Machine-generated**

**Training Languages:** C++, Python, Java  
**Domains:** Algorithmic (e.g., LeetCode-style problems), with evaluation including unseen languages and domains.  

**Dataset sizes (approx.):**

- Train: 500K samples (238K human, 262K machine-generated)  
- Validation: 100K samples  

**Official metric:**  
- **Macro F1-score** (used for leaderboard ranking)

---

### ðŸ”¹ Subtask B â€” Multi-Class LLM Family Authorship Detection

**Goal:**  
Given a code snippet, predict its **author family**:

- **Human**  
- One of **10 LLM families**:
  - DeepSeek-AI  
  - Qwen  
  - 01-ai  
  - BigCode  
  - Gemma  
  - Phi  
  - Meta-LLaMA  
  - IBM-Granite  
  - Mistral  
  - OpenAI  

**Dataset sizes (approx.):**

- Train: 500K samples (highly imbalanced: many human samples, fewer per LLM family)  
- Validation: 100K samples  

**Official metric:**  
- **Macro F1-score**

---

### ðŸ”¹ Subtask C â€” Hybrid + Adversarial Code Classification

**Goal:**  
Classify each code snippet into one of **four** categories:

1. **Human-written**  
2. **Machine-generated**  
3. **Hybrid** â€“ partially written or completed by an LLM  
4. **Adversarial** â€“ generated to mimic human style (e.g., via adversarial prompts or RLHF)  

**Dataset sizes (approx.):**

- Train: 900K samples  
- Validation: 200K samples  

**Official metric:**  
- **Macro F1-score**

semeval-2026-task13-full/
â”‚
â”œâ”€â”€ common/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_utils.py       # Shared data loading utilities
â”‚   â”œâ”€â”€ metrics.py          # Accuracy, Macro-F1, weighted-F1, reports
â”‚   â””â”€â”€ plotting.py         # Confusion matrices & visualizations
â”‚
â”œâ”€â”€ task_a/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ models.py        # TF-IDF, classical ML models, transformers
â”‚   â”‚   â”œâ”€â”€ train_utils.py   # Training loops & dataloaders
â”‚   â”‚   â”œâ”€â”€ eval_utils.py    # Evaluation helpers
â”‚   â”‚   â””â”€â”€ inference.py     # Submission CSV generator
â”‚   â”œâ”€â”€ experiments/
â”‚   â”‚   â”œâ”€â”€ run_tfidf_baseline.py
â”‚   â”‚   â”œâ”€â”€ run_transformer.py
â”‚   â”‚   â””â”€â”€ run_ensemble.py
â”‚   â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ results/
â”‚   â”‚   â”œâ”€â”€ logs/
â”‚   â”‚   â”œâ”€â”€ plots/
â”‚   â”‚   â””â”€â”€ submissions/
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ raw/             # Parquet files (NOT uploaded to GitHub)
â”‚       â””â”€â”€ processed/
â”‚
â”œâ”€â”€ task_b/                  # Same layout as task_a
â”‚
â”œâ”€â”€ task_c/                  # Same layout as task_a
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ EDA.ipynb
â”‚   â””â”€â”€ model_analysis.ipynb
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_models.sh
â”‚   â”œâ”€â”€ prepare_data.py
â”‚   â”œâ”€â”€ evaluate_all.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
