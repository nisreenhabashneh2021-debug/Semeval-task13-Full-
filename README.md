# SemEval-2026 Task 13 â€“ Full System Solution

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Complete solution for **SemEval-2026 Task 13: Machine-Generated Code Detection and Attribution**, covering all three subtasks with modular, extensible implementations.

## ğŸ“‹ Overview

This repository provides a comprehensive framework for detecting and classifying machine-generated code:

- **Subtask A** â€” Binary Machine-Generated Code Detection
- **Subtask B** â€” Multi-Class LLM Family Authorship Detection
- **Subtask C** â€” Hybrid + Adversarial Code Classification

### Features

- ğŸ”§ **Modular Architecture** â€” Reusable components across all subtasks
- ğŸ¤– **Multiple Model Types** â€” TF-IDF baselines, BERT, CodeBERT, CodeT5-small
- ğŸ“Š **Comprehensive Evaluation** â€” Accuracy, Macro-F1, confusion matrices, detailed reports
- ğŸ”„ **Reproducible Experiments** â€” Consistent structure and configuration-driven experiments
- ğŸ“ˆ **Visualization Tools** â€” Built-in plotting and analysis utilities

## ğŸ—‚ï¸ Project Structure


semeval-2026-task13-full/
â”‚
â”œâ”€â”€ common/                      # Shared utilities across all tasks
â”‚   â”œâ”€â”€ _init_.py
â”‚   â”œâ”€â”€ data_utils.py           # Data loading and preprocessing
â”‚   â”œâ”€â”€ metrics.py              # Evaluation metrics (Accuracy, Macro-F1)
â”‚   â””â”€â”€ plotting.py             # Visualization utilities
â”‚
â”œâ”€â”€ task_a/                      # Subtask A: Binary Detection
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ _init_.py
â”‚   â”‚   â”œâ”€â”€ models.py           # Model implementations
â”‚   â”‚   â”œâ”€â”€ train_utils.py      # Training loops and utilities
â”‚   â”‚   â”œâ”€â”€ eval_utils.py       # Evaluation functions
â”‚   â”‚   â””â”€â”€ inference.py        # Submission generation
â”‚   â”œâ”€â”€ experiments/
â”‚   â”‚   â”œâ”€â”€ run_tfidf_baseline.py
â”‚   â”‚   â”œâ”€â”€ run_transformer.py
â”‚   â”‚   â””â”€â”€ run_ensemble.py
â”‚   â”œâ”€â”€ configs/                # Configuration files (YAML/JSON)
â”‚   â”œâ”€â”€ results/
â”‚   â”‚   â”œâ”€â”€ logs/               # Training logs
â”‚   â”‚   â”œâ”€â”€ plots/              # Visualizations
â”‚   â”‚   â””â”€â”€ submissions/        # Competition submissions
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ raw/                # Original datasets
â”‚       â””â”€â”€ processed/          # Preprocessed data
â”‚
â”œâ”€â”€ task_b/                      # Subtask B: Multi-Class Attribution
â”‚   â””â”€â”€ [Same structure as task_a]
â”‚
â”œâ”€â”€ task_c/                      # Subtask C: Hybrid Classification
â”‚   â””â”€â”€ [Same structure as task_a]
â”‚
â”œâ”€â”€ notebooks/                   # Jupyter notebooks for analysis
â”œâ”€â”€ scripts/                     # Utility scripts
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
