# SemEval-2026 Task 13 â€“ Full System Solution

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Complete solution for **SemEval-2026 Task 13: Machine-Generated Code Detection and Attribution**, covering all three subtasks with modular, extensible implementations.

## ğŸ“‹ Overview

This repository provides a comprehensive framework for detecting and classifying machine-generated code:

- **Subtask A** â€” Binary Machine-Generated Code Detection
- **Subtask B** â€” Multi-Class LLM Family Authorship Detection
- **Subtask C** â€” Hybrid + Adversarial Code Classification

### Features

- ğŸ”§ **Modular Architecture** â€” Reusable components across all subtasks
- ğŸ¤– **Multiple Model Types** â€” TF-IDF baselines, BERT, CodeBERT, CodeT5-small
- ğŸ“Š **Comprehensive Evaluation** â€” Accuracy, Macro-F1, confusion matrices, detailed reports
- ğŸ”„ **Reproducible Experiments** â€” Consistent structure and configuration-driven experiments
- ğŸ“ˆ **Visualization Tools** â€” Built-in plotting and analysis utilities

## ğŸ—‚ï¸ Project Structure


semeval-2026-task13-full/
â”‚
â”œâ”€â”€ common/                      # Shared utilities across all tasks
â”‚   â”œâ”€â”€ _init_.py
â”‚   â”œâ”€â”€ data_utils.py           # Data loading and preprocessing
â”‚   â”œâ”€â”€ metrics.py              # Evaluation metrics (Accuracy, Macro-F1)
â”‚   â””â”€â”€ plotting.py             # Visualization utilities
â”‚
â”œâ”€â”€ task_a/                      # Subtask A: Binary Detection
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ _init_.py
â”‚   â”‚   â”œâ”€â”€ models.py           # Model implementations
â”‚   â”‚   â”œâ”€â”€ train_utils.py      # Training loops and utilities
â”‚   â”‚   â”œâ”€â”€ eval_utils.py       # Evaluation functions
â”‚   â”‚   â””â”€â”€ inference.py        # Submission generation
â”‚   â”œâ”€â”€ experiments/
â”‚   â”‚   â”œâ”€â”€ run_tfidf_baseline.py
â”‚   â”‚   â”œâ”€â”€ run_transformer.py
â”‚   â”‚   â””â”€â”€ run_ensemble.py
â”‚   â”œâ”€â”€ configs/                # Configuration files (YAML/JSON)
â”‚   â”œâ”€â”€ results/
â”‚   â”‚   â”œâ”€â”€ logs/               # Training logs
â”‚   â”‚   â”œâ”€â”€ plots/              # Visualizations
â”‚   â”‚   â””â”€â”€ submissions/        # Competition submissions
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ raw/                # Original datasets
â”‚       â””â”€â”€ processed/          # Preprocessed data
â”‚
â”œâ”€â”€ task_b/                      # Subtask B: Multi-Class Attribution
â”‚   â””â”€â”€ [Same structure as task_a]
â”‚
â”œâ”€â”€ task_c/                      # Subtask C: Hybrid Classification
â”‚   â””â”€â”€ [Same structure as task_a]
â”‚
â”œâ”€â”€ notebooks/                   # Jupyter notebooks for analysis
â”œâ”€â”€ scripts/                     # Utility scripts
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md


## ğŸš€ Getting Started

### Prerequisites

- Python 3.8 or higher
- CUDA-compatible GPU (recommended for transformer models)

### Installation

1. Clone the repository:
bash
git clone https://github.com/yourusername/semeval-2026-task13-full.git
cd semeval-2026-task13-full


2. Install dependencies:
bash
pip install -r requirements.txt


3. Download and prepare the data:
bash
python scripts/download_data.py
python scripts/preprocess_data.py


## ğŸ’» Usage

### Running Experiments

Each subtask follows the same experimental structure:

#### TF-IDF Baseline
bash
python task_a/experiments/run_tfidf_baseline.py --config task_a/configs/tfidf_config.yaml


#### Transformer Models
bash
python task_a/experiments/run_transformer.py \
    --model bert-base-uncased \
    --epochs 10 \
    --batch_size 16 \
    --learning_rate 2e-5


#### Ensemble Methods
bash
python task_a/experiments/run_ensemble.py --config task_a/configs/ensemble_config.yaml


### Generating Submissions

bash
python task_a/src/inference.py \
    --model_path task_a/results/best_model.pt \
    --test_data data/test.csv \
    --output task_a/results/submissions/submission.csv


## ğŸ“Š Model Performance

| Subtask | Model | Accuracy | Macro-F1 |
|---------|-------|----------|----------|
| A | TF-IDF Baseline | TBD | TBD |
| A | CodeBERT | TBD | TBD |
| B | CodeT5-small | TBD | TBD |
| C | Ensemble | TBD | TBD |

## ğŸ”§ Configuration

Each experiment can be configured using YAML files in the `configs/` directory:

yaml
model:
  name: "microsoft/codebert-base"
  max_length: 512

training:
  epochs: 10
  batch_size: 16
  learning_rate: 2e-5
  warmup_steps: 500

data:
  train_path: "data/processed/train.csv"
  val_path: "data/processed/val.csv"


## ğŸ“ˆ Results and Visualization

Results are automatically saved to the `results/` directory for each task:
- Training logs in `logs/`
- Plots and confusion matrices in `plots/`
- Submission files in `submissions/`

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- SemEval-2026 Task 13 organizers
- Hugging Face for transformer implementations
- The open-source NLP community

## ğŸ“§ Contact

For questions or feedback, please open an issue or contact [your-email@example.com](mailto:your-email@example.com).

## ğŸ“š Citation

If you use this code in your research, please cite:

bibtex
@inproceedings{yourname2026semeval,
  title={Your System Description for SemEval-2026 Task 13},
  author={Your Name},
  booktitle={Proceedings of SemEval-2026},
  year={2026}
}


---

**Note:** This is a work in progress. Results and implementations will be updated as experiments are completed.